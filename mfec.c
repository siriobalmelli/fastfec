#include <mfec.h>

/*	MultiFEC

SYNOPSIS
A system for the increase of recovery probability over large datasets
	without the delay occasioned by large block FEC codecs.

This is accomplished by:
-	Very fast FEC encode/decode (see ffec.c).
-	Partially overlapping the "source" or "k" symbol regions of multiple
		smaller, independent FEC matrices.
-	Using virtual memory to map overlapping and non-contiguous regions
		of an underlying circular buffer into "flat" virt. memory regions
		for FEC computation.
-	A continuous (windowing) circular buffer arrangement:
		every new 'page' is multiplexed into the previous 'width' pages,
		"dropping" one old page off the end of the window.
-	Pre-seeding initial pages at beginning of operation
		(absence of previously encoded data) with predictably random pages,
		so protection is homogeneous through the operation lifecycle.


NOMENCLATURE
'page'		:	A block of source symbols, the unit in which source symbol
				memory is computed/handled.
			Must be a multiple of the system page size.
			FEC encode/decode is done in incremental blocks of 'page' size:
				'page' effectively sets the DELAY of the codec.

'width'		:	How many separate FEC matrices include the symbols in
				a page (conceptually, the "depth" of overlap).
			Width is ALSO the amount of pages which make up
				the "source" region of each FEC matrix.

'matrix'	:	A set of 'width' pages artificially concatenated into a contiguous
				virtual memory address space (pointing at the underlying ringbuffer)
				and forming the "source" or "n" region of a FEC matrix.
			The "k" or "source" symbols partially overlap other matrices.
			The "p" or "parity" symbols (generated by FEC) are
				unique to each matrix.

'book'		:	The metadata and tracking structure for one matrix.
			While 'matrix' and 'book' can (loosely) be considered interchangeable,
				the former strictly refers to the SYMBOLS,
				while the latter refers to the metadata.

'span'		:	This is 'width * 2 -1' and is the size of all ringbuffer
				allocations (source regions, repair regions, FEC structs, etc).

'seq_no'	:	Monotonic sequence counter of FEC matrices,
				also used as the random seed for FEC generation.


MEMORY LAYOUT
	width	:	3
	span	:	width * 2 -1
	
seq		source_pages				repair region

0	Seed0	Seed1	Page0				R0
1		Seed1	Page0	Page1			R1
2			Page0	Page1	Page2		R2
3	Page3			Page1	Page2		R3	inval. seq0
4	Page3	Page4			Page2		R4	inval. seq1
[cycle repeats as `seq % span`]
5	Page3	Page4	Page5				R0	inval. seq2
6		Page4	Page5	Page6			R1	inval. seq3
7			Page5	Page6	Page7		R2	inval. seq4
8	Page8			Page6	Page7		R3	inval. seq5
9	Page8	Page9			Page7		R4	inval. seq6

TODO:
-	Can we use 2/4MB hugepages?
*/


#ifdef Z_BLK_LVL
#undef Z_BLK_LVL
#endif
#define Z_BLK_LVL 0
/* debug levels:
	2: ringbuffer; mmap into ringbuffer
	3: symbol dec & propagate
*/


/*	mfec_source_map()
Assemble (map) 'width' pages from the underlying ring (may roll over somewhere in the middle)
	into a contiguous virtual memory region.

Returns pointer to said region, NULL on error.
We ASSUME returned region must be munmap()ed, but valgrind doesn't complain if not (?!?)

NOTE: 'page_idx' means the FIRST page in the mapping,
	which will then extend 'width' pages.
'page_idx' is 'seq % span'

Populates 'last_offt' with the offset (in the underlying ringbuffer, not the
	this assembled memory region) of the "data" region,
	which callers may wish to splice() into.
TODO: can we splice into this assembled region using the FD of the underlying
	ringbuffer?

TODO: in cases of single-shot (contiguous) regions of the underlying ring,
	return an address in the ring itself (save TLB trashing);
	but then make sure it doesn't get unmapped on inval.
*/
void		*mfec_source_map(struct mfec_hp *hp, uint64_t page_idx, off_t *last_offt)
{
	void *ret = NULL;

	int map_len = hp->span - page_idx;

	/* can mmap all in one shot */
	// TODO: why map at all - how about returning an underlying pointer?
	if (map_len >= hp->width) {
		Z_die_if((
			ret = mmap(NULL, hp->width * mfec_pg(hp),
					PROT_READ | PROT_WRITE,
					MAP_SHARED,
					hp->ring_fd,
					page_idx * mfec_pg(hp))
			) == MAP_FAILED, "");
		if (last_offt)
			*last_offt = (page_idx + hp->width -1) * mfec_pg(hp);

	/* must split map into 2 operations to roll around end of ring */
	} else {
		Z_die_if((
			ret = mmap(NULL, mfec_pg(hp) * map_len,
					PROT_READ | PROT_WRITE,
					MAP_SHARED,
					hp->ring_fd,
					page_idx * mfec_pg(hp))
			) == MAP_FAILED, "");
		Z_die_if((
			/* requested virtual mem addr is contiguous to previous map */
			mmap(ret + mfec_pg(hp) * map_len, 
				mfec_pg(hp) * (hp->width - map_len),
				PROT_READ | PROT_WRITE,
				MAP_SHARED | MAP_FIXED,
				hp->ring_fd,
				/* we just rolled over, so offset is clearly 0 */
				0)
			) == MAP_FAILED, "");
		if (last_offt)
			*last_offt = (hp->width - map_len -1) * mfec_pg(hp);
	}

	Z_inf(2, "assemble idx %ld @0x%lx; last_offt %ld", page_idx, (uint64_t)ret, *last_offt);
	return ret;
out:
	if (ret && ret != MAP_FAILED)
		munmap(ret, mfec_pg(hp) * hp->width); /* unmap the maximum possible amount */
	return NULL;
}


/*	mfec_bk_inval()
Free memory mapped by mfec_source_map().
*/
void		mfec_bk_inval	(struct mfec_bk *bk)
{
	bk->seq_no = -1;
	// TODO: must not unmap if address is in the_one_ring
	if (bk->fi.source) {
		munmap(bk->fi.source, mfec_pg(bk->hp) * bk->hp->width);
		bk->fi.source = NULL;
	}
	int rc;
	if (bk->j1_done)
		J1FA(rc, bk->j1_done);
	if (bk->j1_pend)
		J1FA(rc, bk->j1_pend);
}

/*	mfec_bk_get()
Get pointer to book ONLY if seq_no matches.

Returns NULL on mismatch; the world may have moved on, you know - ka and all that.
*/
struct mfec_bk	*mfec_bk_get	(struct mfec_hp *hp, uint64_t seq_no)
{
	struct mfec_bk *ret = &hp->books[seq_no % hp->span];
	/* check 'source' also, because we may be given the "invalid" -1 case
		as a valid input, when decode_multi_() tries to recurse back from 0.
	*/
	if (ret->seq_no != seq_no || !ret->fi.source)
		ret = NULL;
	return ret;
}

/*	mfec_bk_next()
Invalidate the furthest book whose symbols we would clobber, at 'seq_no+width-1'
	(which is the same as 'seq_no-width', accounting for rollover
	across a set of 'span' books).
Initialize new book @'seq_no'.
'seq_no++'
*/
struct mfec_bk	*mfec_bk_next	(struct mfec_hp *hp)
{
	Z_die_if(!hp, "sanity");

	/* Inval the book we will now clobber
		(our "data" region is their first page).
	*/
	mfec_bk_inval(&hp->books[(hp->seq_no + hp->width - 1) % hp->span]);

	/* init new book */
	struct mfec_bk *ret = &hp->books[hp->seq_no % hp->span];
	ret->seq_no = hp->seq_no++;
	ret->fi.source = mfec_source_map(hp, ret->seq_no % hp->span, &ret->data_offt);
	ret->hp = hp;
	Z_die_if((
		ffec_init(&hp->fp, &ret->fi,
			mfec_pg(hp) * hp->width,
			ret->fi.source, ret->fi.parity, ret->fi.scratch,
			hp->dir,
			/* use 'seq_no +1' as the RNG seed for FFEC */
			PCG_RAND_S2, ret->seq_no + 1)
		), "");

	/* if this is the first book (TX || RX): seed */
	if (!ret->seq_no) {
		pcg_randset(ret->fi.source, mfec_pg(hp) * (hp->width -1),
				PCG_RAND_S1, PCG_RAND_S2);
		/* if decoding, mark symbols in FFEC matrix */
		if (hp->dir == decode) {
			for (uint64_t i=0; i < hp->syms_page * (hp->width -1); i++)
				ffec_decode_sym(&hp->fp, &ret->fi, NULL, i);
		}

	/* otherwise for RX/decode: check previous book for all symbols already decoded
		and mark them decoded here.
	*/
	} else if (hp->dir == decode) {
		struct mfec_bk *prev = mfec_bk_get(hp, ret->seq_no -1);
		for (uint32_t i=mfec_esi_walk(hp, 0, 1); i < prev->fi.cnt.n; i++) {
			if (ffec_test_esi(&prev->fi, i))
				ffec_decode_sym(&hp->fp, &ret->fi, NULL, mfec_esi_walk(hp, i, -1));
		}
	}

	/* NOTE: TX/encode expects user to prepare "data" and then call mfec_encode() */

	return ret;
out:
	return NULL;
}


/*	mfec_encode()
Generate repair symbols for 'bk'.
ALL symbols are encoded simultaneously, there is no facility for partial encoding.

If 'data' is provided, splice() the "data" (last) page from there;
	otherwise assume it was written to directly by caller,
	using pointer obtained from mfec_bk_data().
ASSUME 'data' is of size 'mfec_pg()'

Generate and return an array of uint32_t ESIs as the (randomized) order
	in which to send symbols.
Caller must free returned array.
Returns NULL on error.
*/
uint32_t	*mfec_encode	(struct mfec_bk *bk, void *data)
{
	int err_cnt = 0;
	uint32_t *ret = NULL;
	Z_die_if(!bk || bk->hp->dir != encode, "sanity");

	// TODO: splicing gives speed improvement?
	Z_die_if(data, "splicing not implemented");

	err_cnt = ffec_encode(&bk->hp->fp, &bk->fi);

	/* alloc and return randomized sequence of ESIs */
	Z_die_if(!(
		ret = malloc(mfec_bk_txesi_cnt(bk) * sizeof(uint32_t))
		), "");
	ffec_esi_rand(&bk->fi, ret, bk->hp->syms_page * (bk->hp->width - 1));
out:
	return ret;
}

/*	mfec_sym_propagate()
Return number of adjanced books which were "dirtied" with
	propagated symbol (aka: they now need to decode).
*/
uint64_t	mfec_sym_propagate(struct mfec_bk *bk, uint32_t	esi)
{
	uint64_t dirtied = 0;
	struct mfec_hp *hp = bk->hp; /* legibility */
	
	/* walk possible range of adjacent books */
	for (int64_t walk = 1L - (int64_t)hp->width; walk < hp->width; walk++) {
		/* ... skipping self */
		if (!walk)
			continue;
		/* Walk ESI in OPPOSITE direction:
			symbol n has a higher ESI in an earlier matrix
		Skip books where symbol is out of range.
		*/
		int64_t EE = mfec_esi_walk(hp, esi, walk * -1);
		if (EE < 0 || EE >= bk->fi.cnt.k) {
			continue;
		}
		/* Try and get a valid book */
		struct mfec_bk *cur = mfec_bk_get(hp, (int64_t)bk->seq_no + walk);
		if (!cur)
			continue;
		
		/* propagate symbol only if not marked "done" */
		int rc;
		J1T(rc, cur->j1_done, EE);
		if (!rc) {
			Z_inf(3, "\tpropagate %d@%ld -> %ld@%ld",
					esi, bk->seq_no, EE, cur->seq_no);
			J1S(rc, cur->j1_pend, EE);
			dirtied++;
		}
	}
	return dirtied;
}

/*	mfec_decode()
Decode 'symbol' at 'esi' for 'bk'.
Then find all the overlapping matrices where this decoded symbol could be propagated,
	and decode it there as well.
Decoding may generate new symbols - propagate these as well (which may generate yet
	new symbols to be propagated and so on).

returns the number of symbols left to decode for 'bk'.

NOTE - propagation:
	example @ width=3; span=5
						walk	j1_pend[] | j1_done[]
			P1			-2	0
			P1	P2		-1	1
	we here ->	P1	P2	P3	0	2
				P2	P3	1	3
					P3	2	4
*/
uint32_t	mfec_decode	(struct mfec_bk	*bk, void *symbol, uint32_t esi)
{
	/* bail if 'seq_no' doesn't yield a valid book */
	if (!bk)
		return -1;
	/* if 'esi' already decoded, return status */
	if (ffec_test_esi(&bk->fi, esi))
		return bk->fi.cnt.k - bk->fi.cnt.k_decoded;
	Z_inf(3, "dec");

	struct mfec_hp *hp = bk->hp; /* legibility */

	/* move first symbol into matrix */
	void *sym = ffec_get_sym(&hp->fp, &bk->fi, esi);
	if (sym != symbol)
		memcpy(sym, symbol, hp->fp.sym_len);
	/* mark it pending */
	int rc;
	Word_t idx = esi;
	J1S(rc, bk->j1_pend, idx);

	/* loop until ALL adjacent matrices (and self) have NO pending symbols */
	uint64_t dirty;
	do {
		dirty = 0;

		for (int64_t walk = 1L - (int64_t)hp->width; walk < hp->width; walk++) {
			/* skip invalid books */
			struct mfec_bk *cur = mfec_bk_get(hp, bk->seq_no + walk);	
			if (!cur)
				continue;
			idx = 0;
			J1F(rc, cur->j1_pend, idx);
			while (rc) {
				/* DECODE! */
				Pvoid_t temp_decoded = NULL;
				Z_inf(3, "\tsym %ld	bk %ld", idx, cur->seq_no);
				ffec_decode_sym_(&hp->fp, &cur->fi, NULL, idx, &temp_decoded);
				
				/* propagate decoded symbols; mark done */
				Word_t temp_idx = 0;
				J1F(rc, temp_decoded, temp_idx);
				while (rc) {
					J1S(rc, cur->j1_done, temp_idx);
					dirty += mfec_sym_propagate(cur, temp_idx);
					/* unset; next */
					J1U(rc, temp_decoded, temp_idx);
					J1N(rc, temp_decoded, temp_idx);
				}

				/* unset; get next pending */
				J1U(rc, cur->j1_pend, idx);
				J1N(rc, cur->j1_pend, idx);
			}
		}
	} while (dirty);

	/* return number of symbols left to decode */
	return bk->fi.cnt.k - bk->fi.cnt.k_decoded;
}


/*	mfec_hp_init()
*/
int		mfec_hp_init	(struct mfec_hp *hp, uint32_t width, uint64_t syms_page,
				enum ffec_direction dir, struct ffec_params fp,
				uint64_t seq_no)
{
	int err_cnt = 0;
	Z_die_if(!hp, "sanity");
	memset(hp, 0x0, sizeof(*hp));

	hp->width = width;
	hp->span = hp->width * 2 - 1;	/* every region/array is of length 'span' */
	hp->syms_page = syms_page;
	hp->seq_no = seq_no;

	/* ffec params */
	hp->fp.fec_ratio = fp.fec_ratio;
	hp->fp.sym_len = fp.sym_len;
	hp->dir = dir;
	Z_die_if((
		ffec_calc_lengths(&hp->fp, mfec_pg(hp) * hp->width, &hp->fs, dir)
		), "");

	/* verify that quested size parameters give a multiple of pagesize */
	size_t pgsz = getpagesize();
	Z_die_if(mfec_pg(hp) / pgsz * pgsz != mfec_pg(hp),
		"proposed pagesize %ld %% %ld system pagesize = %ld",
		mfec_pg(hp), pgsz, mfec_pg(hp) % pgsz);
	/* mmap() will complain if syms_page < 1024 */
	Z_die_if(syms_page < 1024, "");

	/* one mmap to rule them all ... and at process end unmap them
	Physical layout:
		N = width * 2 -2
		src0	...	srcN	par0	...	parN	scr0	...	scrN
	oh, and make that map a multiple of the system pagesize
	 */
	hp->the_one_ring.iov_base = 0;
	hp->the_one_ring.iov_len = next_mult64((hp->fs.source_sz + hp->fs.parity_sz + hp->fs.scratch_sz)
							* (uint64_t)hp->span,
						pgsz);
	Z_die_if((
		hp->ring_fd = sbfu_tmp_map(&hp->the_one_ring, "./")
		) < 1, "");
	Z_inf(2, "map ring @ 0x%lx", (uint64_t)hp->the_one_ring.iov_base);
		
	/* Allocate and invalidate book structs */
	hp->books = calloc(hp->span, sizeof(struct mfec_bk));
	for (size_t i=0; i < hp->span; i++)
		mfec_bk_inval(&hp->books[i]);

	/* Parity and scratch regions are immutable,
		set the first pointer and then cycle through successive ones to increment.
	 */
	hp->books[0].fi.parity = hp->the_one_ring.iov_base + hp->fs.source_sz * hp->span;
	hp->books[0].fi.scratch = hp->books[0].fi.parity + hp->fs.parity_sz * hp->span;
	for (int i = 1; i < hp->span; i++) {
		hp->books[i].fi.parity = hp->books[i-1].fi.parity + hp->fs.parity_sz;
		hp->books[i].fi.scratch = hp->books[i-1].fi.scratch + hp->fs.scratch_sz;
	}

out:
	return err_cnt;
}

/*	mfec_hp_clean()
*/
void		mfec_hp_clean	(struct mfec_hp *hp)
{
	if (!hp)
		return;

	if (hp->books) {
		for (int i=0; i < hp->span; i++)
			mfec_bk_inval(&hp->books[i]);
		free(hp->books);
	}

	if (hp->ring_fd > 0)
		sbfu_unmap(hp->ring_fd, &hp->the_one_ring);
	hp->ring_fd = 0;
}

#undef Z_BLK_LVL
#define Z_BLK_LVL 0
